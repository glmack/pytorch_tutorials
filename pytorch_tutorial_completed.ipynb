{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Introduction to PyTorch\n",
    "***********************\n",
    "\n",
    "Introduction to Torch's tensor library\n",
    "======================================\n",
    "\n",
    "- All of deep learning is computations on tensors\n",
    "- tensors: generalizations of a matrix that can be indexed in more than 2 dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee26e38210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.Tensor` create tensors\n",
    "\n",
    "- Tensors can be created from Python lists with the torch.Tensor()\n",
    "function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.Tensor(V_data)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.Tensor(M_data)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.Tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a 3D tensor anyway? Think about it like this. If you have a\n",
    "vector, indexing into the vector gives you a scalar. If you have a\n",
    "matrix, indexing into the matrix gives you a vector. If you have a 3D\n",
    "tensor, then indexing into the tensor gives you a matrix!\n",
    "\n",
    "A note on terminology:\n",
    "- \"tensor\" in this tutorial refers to any torch.Tensor object. Matrices and vectors are special cases of\n",
    "torch.Tensors, where their dimension is 1 and 2 respectively. When I am talking about 3D tensors, I will explicitly use the term \"3D tensor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "V\n",
    "V[0]\n",
    "V[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a Python number from it\n",
    "V\n",
    "V[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index into M and get a vector\n",
    "M\n",
    "M[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into T and get a matrix\n",
    "T\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datatypes of tensors\n",
    "You can also create tensors of other datatypes. The default, as you can\n",
    "see, is Float. To create a tensor of integer types, try\n",
    "torch.LongTensor(). Check the documentation for more data types, but\n",
    "Float and Long will be the most common.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[5, 6],\n",
       "         [7, 8]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(T_data) # creates a tensor of float datatype\n",
    "torch.LongTensor(T_data) # creates a tensor of integer datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.randn` create a tensor with random data and the supplied dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with Tensors\n",
    "- operate on tensors in the ways you would expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `the documentation <http://pytorch.org/docs/torch.html>`__ for a\n",
    "complete list of the massive number of operations available to you. They\n",
    "expand beyond just mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.cat(tensor_1, tensor_2)` concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4791,  1.3790,  2.5286,  0.4107, -0.9880],\n",
       "        [-0.9081,  0.5423,  0.1103, -2.2590,  0.6067]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1383,  0.8310, -0.2477, -0.8029,  0.2366],\n",
       "        [ 0.2857,  0.6898, -0.6331,  0.8795, -0.6842],\n",
       "        [ 0.4533,  0.2912, -0.8317, -0.5525,  0.6355]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4791,  1.3790,  2.5286,  0.4107, -0.9880],\n",
       "        [-0.9081,  0.5423,  0.1103, -2.2590,  0.6067],\n",
       "        [-0.1383,  0.8310, -0.2477, -0.8029,  0.2366],\n",
       "        [ 0.2857,  0.6898, -0.6331,  0.8795, -0.6842],\n",
       "        [ 0.4533,  0.2912, -0.8317, -0.5525,  0.6355]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "x_1\n",
    "\n",
    "y_1 = torch.randn(3, 5)\n",
    "y_1\n",
    "\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1115,  0.3501, -0.7703],\n",
       "        [-0.1473,  0.6272,  1.0935]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0939,  1.2381, -1.3459,  0.5119, -0.6933],\n",
       "        [-0.1668, -0.9999, -1.6476,  0.8098,  0.0554]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "x_2\n",
    "y_2 = torch.randn(2, 5)\n",
    "y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1115,  0.3501, -0.7703,  0.0939,  1.2381, -1.3459,  0.5119, -0.6933],\n",
       "        [-0.1473,  0.6272,  1.0935, -0.1668, -0.9999, -1.6476,  0.8098,  0.0554]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.view()` - Reshape Tensors\n",
    "- .view() method to reshape a tensor.\n",
    "- many neural network components expect their inputs to have\n",
    "a certain shape. Often you will need to reshape before passing your data\n",
    "to the component.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation Graphs and Automatic Differentiation\n",
    "================================================\n",
    "\n",
    "- concept of a computation graph is essential to efficient deep learning programming\n",
    "- it allows you to not have to write theback propagation gradients yourself. \n",
    "- a computation graph is simply a specification of how your data is combined to give you the output. \n",
    "- the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
    "\n",
    "- fundamental flag ``requires_grad``.\n",
    "\n",
    "- What is stored in the torch.Tensor objects we were creating above? \n",
    "    - Obviously the data and the shape, and maybe a few other things. \n",
    "    - But when we added two tensors together, we got an output tensor. \n",
    "    = All this output tensor knows is its data and shape. It has no idea that it was the sum of two other tensors (it could have been read in from a file, it could be the result of some other operation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``requires_grad=True`` - tensor object keeps track of how it was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with requires_grad=True, can do all the operations\n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7fee260166d8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but z knows something extra.\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors know what created them. \n",
    "- z knows:\n",
    "    - that it wasn't read in from a file\n",
    "    - it wasn't the result of a multiplication or exponential\n",
    "    - if you keep following z.grad_fn, you will find yourself at x and y.\n",
    "\n",
    "But how does that help us compute a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(21., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x7fee25b8bb00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "z\n",
    "s = z.sum()\n",
    "s\n",
    "s.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the derivative of this sum with respect to the first component of x?\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "- s knows that it was created as a sum of the tensor z\n",
    "- z knows that it was the sum x + y. So\n",
    "\n",
    "\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n",
    "\n",
    "And so s contains enough information to determine that the derivative\n",
    "we want is 1!\n",
    "\n",
    "- this glosses over the challenge of how to actually compute that derivative\n",
    "- the point here is that s is carrying along enough information that it is possible to compute it\n",
    "- the developers of Pytorch program the sum() and + operations to know how to compute their gradients, and run the back propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have Pytorch compute the gradient, and see that we were right:\n",
    "(note if you run this block multiple times, the gradient will increment.\n",
    "That is because Pytorch *accumulates* the gradient into the .grad\n",
    "property, since for many models this is very convenient.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.backward()` will run backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s\n",
    "s.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what is going on in the block below is crucial for being a\n",
    "successful programmer in deep learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default, user-created Tensors have ``requires_grad=False``\n",
    "x.requires_grad, y.requires_grad\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7fee259e0630>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so you can't backprop through z - #TODO (Lee) - unclear\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.requires_grad_()` - changes existing Tensor's `requires_grad`  flag in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7fee25aa15c0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.detach()` breaks Tensor away from its past history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "\n",
    "print(new_z.grad_fn)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrapping the code block in``with torch.no_grad():`` also stop autograd from tracking history on Tensors with ``.requires_grad``=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
